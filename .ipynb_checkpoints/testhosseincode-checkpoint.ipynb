{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2de563c-e4cc-4c00-8e2f-ab8a05ac445f",
   "metadata": {},
   "source": [
    "## create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8c80f7-938d-4bc1-b3ca-839b724b9ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 The Kubeflow Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the `License.\n",
    "\"\"\"Sample pipeline for passing data in KFP.\"\"\"\n",
    "from typing import Dict, List\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "\n",
    "@component\n",
    "def preprocess(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # An input parameter of type dict.\n",
    "    input_dict_parameter: Dict[str, int],\n",
    "    # An input parameter of type list.\n",
    "    input_list_parameter: List[str],\n",
    "    # Use Output[T] to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset_one: Output[Dataset],\n",
    "    # A locally accessible filepath for another output artifact of type\n",
    "    # `Dataset`.\n",
    "    output_dataset_two_path: OutputPath('Dataset'),\n",
    "    # A locally accessible filepath for an output parameter of type string.\n",
    "    output_parameter_path: OutputPath(str),\n",
    "    # A locally accessible filepath for an output parameter of type bool.\n",
    "    output_bool_parameter_path: OutputPath(bool),\n",
    "    # A locally accessible filepath for an output parameter of type dict.\n",
    "    output_dict_parameter_path: OutputPath(Dict[str, int]),\n",
    "    # A locally accessible filepath for an output parameter of type list.\n",
    "    output_list_parameter_path: OutputPath(List[str]),\n",
    "):\n",
    "    \"\"\"Dummy preprocessing step.\"\"\"\n",
    "\n",
    "    # Use Dataset.path to access a local file path for writing.\n",
    "    # One can also use Dataset.uri to access the actual URI file path.\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    # OutputPath is used to just pass the local file path of the output artifact\n",
    "    # to the function.\n",
    "    with open(output_dataset_two_path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_parameter_path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_bool_parameter_path, 'w') as f:\n",
    "        f.write(\n",
    "            str(True))  # use either `str()` or `json.dumps()` for bool values.\n",
    "\n",
    "    import json\n",
    "    with open(output_dict_parameter_path, 'w') as f:\n",
    "        f.write(json.dumps(input_dict_parameter))\n",
    "\n",
    "    with open(output_list_parameter_path, 'w') as f:\n",
    "        f.write(json.dumps(input_list_parameter))\n",
    "\n",
    "\n",
    "@component\n",
    "def train(\n",
    "    # Use InputPath to get a locally accessible path for the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_one_path: InputPath('Dataset'),\n",
    "    # Use Input[T] to get a metadata-rich handle to the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_two: Input[Dataset],\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use Output[T] to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    model: Output[Model],\n",
    "    # An input parameter of type bool.\n",
    "    input_bool: bool,\n",
    "    # An input parameter of type dict.\n",
    "    input_dict: Dict[str, int],\n",
    "    # An input parameter of type List[str].\n",
    "    input_list: List[str],\n",
    "    # An input parameter of type int with a default value.\n",
    "    num_steps: int = 100,\n",
    "):\n",
    "    \"\"\"Dummy Training step.\"\"\"\n",
    "    with open(dataset_one_path) as input_file:\n",
    "        dataset_one_contents = input_file.read()\n",
    "\n",
    "    with open(dataset_two.path) as input_file:\n",
    "        dataset_two_contents = input_file.read()\n",
    "\n",
    "    line = (f'dataset_one_contents: {dataset_one_contents} || '\n",
    "            f'dataset_two_contents: {dataset_two_contents} || '\n",
    "            f'message: {message} || '\n",
    "            f'input_bool: {input_bool}, type {type(input_bool)} || '\n",
    "            f'input_dict: {input_dict}, type {type(input_dict)} || '\n",
    "            f'input_list: {input_list}, type {type(input_list)} \\n')\n",
    "    with open(model.path, 'w') as output_file:\n",
    "        for i in range(num_steps):\n",
    "            output_file.write(f'Step {i}\\n{line}\\n=====\\n')\n",
    "\n",
    "    # model is an instance of Model artifact, which has a .metadata dictionary\n",
    "    # to store arbitrary metadata for the output artifact.\n",
    "    model.metadata['accuracy'] = 0.9\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='my-test-pipeline-beta')\n",
    "def pipeline(message: str, input_dict: Dict[str, int] = {'A': 1, 'B': 2}):\n",
    "\n",
    "    preprocess_task = preprocess(\n",
    "        message=message,\n",
    "        input_dict_parameter=input_dict,\n",
    "        input_list_parameter=['a', 'b', 'c'],\n",
    "    )\n",
    "    train_task = train(\n",
    "        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n",
    "        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n",
    "        message=preprocess_task.outputs['output_parameter_path'],\n",
    "        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n",
    "        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n",
    "        input_list=preprocess_task.outputs['output_list_parameter_path'],\n",
    "    )\n",
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "yaml_file = os.path.join(current_directory, 'pipeline.yaml')\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     compiler.Compiler().compile(\n",
    "#         pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5ad21d-9671-4e4d-acd2-f929839ccae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiments': None, 'next_page_token': None, 'total_size': None}\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='http://98.64.205.69')\n",
    "print(client.list_experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a64b5-4a79-4c04-a0c7-cb64a8b5e77c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## upload pipeline with pipeline yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bbe606c-0094-412f-86ff-113c11e70657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'date': 'Thu, 20 Jun 2024 14:59:23 GMT', 'content-length': '483', 'content-type': 'text/plain; charset=utf-8', 'x-envoy-upstream-service-time': '24', 'server': 'envoy'})\nHTTP response body: {\"error_message\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name test ppl name already exists. Please specify a new name\",\"error_details\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name test ppl name already exists. Please specify a new name\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pipeline_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest ppl name\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m kfp\u001b[38;5;241m.\u001b[39mClient()\n\u001b[0;32m----> 5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_uploads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mpipeline_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:71\u001b[0m, in \u001b[0;36mPipelineUploadServiceApi.upload_pipeline\u001b[0;34m(self, uploadfile, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"upload_pipeline  # noqa: E501\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m:rtype: V2beta1Pipeline\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_pipeline_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43muploadfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:170\u001b[0m, in \u001b[0;36mPipelineUploadServiceApi.upload_pipeline_with_http_info\u001b[0;34m(self, uploadfile, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m    168\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/v2beta1/pipelines/upload\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV2beta1Pipeline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:364\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    372\u001b[0m                                                method, path_params,\n\u001b[1;32m    373\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                                _request_timeout,\n\u001b[1;32m    382\u001b[0m                                                _host))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:188\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    190\u001b[0m content_type \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mgetheader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:407\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(url,\n\u001b[1;32m    402\u001b[0m                                     query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    403\u001b[0m                                     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    404\u001b[0m                                     _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    405\u001b[0m                                     _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(url,\n\u001b[1;32m    416\u001b[0m                                 query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    417\u001b[0m                                 headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m                                 _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    421\u001b[0m                                 body\u001b[38;5;241m=\u001b[39mbody)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/rest.py:265\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, post_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    264\u001b[0m          body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/rest.py:224\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'date': 'Thu, 20 Jun 2024 14:59:23 GMT', 'content-length': '483', 'content-type': 'text/plain; charset=utf-8', 'x-envoy-upstream-service-time': '24', 'server': 'envoy'})\nHTTP response body: {\"error_message\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name test ppl name already exists. Please specify a new name\",\"error_details\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name test ppl name already exists. Please specify a new name\"}\n"
     ]
    }
   ],
   "source": [
    " pipeline_file_path = 'pipeline.yaml' # extract it from your database\n",
    " pipeline_name = 'test ppl name'\n",
    "\n",
    " client = kfp.Client()\n",
    " pipeline = client.pipeline_uploads.upload_pipeline(\n",
    "                                pipeline_file_path, name=pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9db5558-7876-4aa0-a1e1-f79316a4c911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 The Kubeflow Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Sample pipeline for passing data in KFP.\"\"\"\n",
    "from typing import Dict, List\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "\n",
    "@component\n",
    "def preprocess(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # An input parameter of type dict.\n",
    "    input_dict_parameter: Dict[str, int],\n",
    "    # An input parameter of type list.\n",
    "    input_list_parameter: List[str],\n",
    "    # Use Output[T] to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset_one: Output[Dataset],\n",
    "    # A locally accessible filepath for another output artifact of type\n",
    "    # `Dataset`.\n",
    "    output_dataset_two_path: OutputPath('Dataset'),\n",
    "    # A locally accessible filepath for an output parameter of type string.\n",
    "    output_parameter_path: OutputPath(str),\n",
    "    # A locally accessible filepath for an output parameter of type bool.\n",
    "    output_bool_parameter_path: OutputPath(bool),\n",
    "    # A locally accessible filepath for an output parameter of type dict.\n",
    "    output_dict_parameter_path: OutputPath(Dict[str, int]),\n",
    "    # A locally accessible filepath for an output parameter of type list.\n",
    "    output_list_parameter_path: OutputPath(List[str]),\n",
    "):\n",
    "    \"\"\"Dummy preprocessing step.\"\"\"\n",
    "\n",
    "    # Use Dataset.path to access a local file path for writing.\n",
    "    # One can also use Dataset.uri to access the actual URI file path.\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    # OutputPath is used to just pass the local file path of the output artifact\n",
    "    # to the function.\n",
    "    with open(output_dataset_two_path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_parameter_path, 'w') as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_bool_parameter_path, 'w') as f:\n",
    "        f.write(\n",
    "            str(True))  # use either `str()` or `json.dumps()` for bool values.\n",
    "\n",
    "    import json\n",
    "    with open(output_dict_parameter_path, 'w') as f:\n",
    "        f.write(json.dumps(input_dict_parameter))\n",
    "\n",
    "    with open(output_list_parameter_path, 'w') as f:\n",
    "        f.write(json.dumps(input_list_parameter))\n",
    "\n",
    "\n",
    "@component\n",
    "def train(\n",
    "    # Use InputPath to get a locally accessible path for the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_one_path: InputPath('Dataset'),\n",
    "    # Use Input[T] to get a metadata-rich handle to the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_two: Input[Dataset],\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use Output[T] to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    model: Output[Model],\n",
    "    # An input parameter of type bool.\n",
    "    input_bool: bool,\n",
    "    # An input parameter of type dict.\n",
    "    input_dict: Dict[str, int],\n",
    "    # An input parameter of type List[str].\n",
    "    input_list: List[str],\n",
    "    # An input parameter of type int with a default value.\n",
    "    num_steps: int = 100,\n",
    "):\n",
    "    \"\"\"Dummy Training step.\"\"\"\n",
    "    with open(dataset_one_path) as input_file:\n",
    "        dataset_one_contents = input_file.read()\n",
    "\n",
    "    with open(dataset_two.path) as input_file:\n",
    "        dataset_two_contents = input_file.read()\n",
    "\n",
    "    line = (f'dataset_one_contents: {dataset_one_contents} || '\n",
    "            f'dataset_two_contents: {dataset_two_contents} || '\n",
    "            f'message: {message} || '\n",
    "            f'input_bool: {input_bool}, type {type(input_bool)} || '\n",
    "            f'input_dict: {input_dict}, type {type(input_dict)} || '\n",
    "            f'input_list: {input_list}, type {type(input_list)} \\n')\n",
    "    with open(model.path, 'w') as output_file:\n",
    "        for i in range(num_steps):\n",
    "            output_file.write(f'Step {i}\\n{line}\\n=====\\n')\n",
    "\n",
    "    # model is an instance of Model artifact, which has a .metadata dictionary\n",
    "    # to store arbitrary metadata for the output artifact.\n",
    "    model.metadata['accuracy'] = 0.9\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='my-test-pipeline-beta')\n",
    "def pipeline(message: str, input_dict: Dict[str, int] = {'A': 1, 'B': 2}):\n",
    "\n",
    "    preprocess_task = preprocess(\n",
    "        message=message,\n",
    "        input_dict_parameter=input_dict,\n",
    "        input_list_parameter=['a', 'b', 'c'],\n",
    "    )\n",
    "    train_task = train(\n",
    "        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n",
    "        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n",
    "        message=preprocess_task.outputs['output_parameter_path'],\n",
    "        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n",
    "        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n",
    "        input_list=preprocess_task.outputs['output_list_parameter_path'],\n",
    "    )\n",
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "yaml_file = os.path.join(current_directory, 'nnncc.yaml')\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "\n",
    "\n",
    "pipeline_file_path = 'nnncc.yaml' # extract it from your database\n",
    "pipeline_name = 'nnncc'\n",
    "\n",
    "client = kfp.Client()\n",
    "pipeline = client.pipeline_uploads.upload_pipeline(\n",
    "                                pipeline_file_path, name=pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d209e5c-2e9a-4e43-9b2b-5b6f88539605",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "print(kfp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b81a5b99-65a2-415f-98ab-c28214f6a607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ID: 97838453-7a91-42b8-a277-c4c550a9e2c4\n",
      "Pipeline Name: [Tutorial] Data passing in python components\n",
      "Pipeline ID: c74e3130-3806-4955-a299-c32f1e22bccd\n",
      "Pipeline Name: [Tutorial] DSL - Control structures\n",
      "Pipeline ID: d0427560-86fd-42f1-a42b-314195292746\n",
      "Pipeline Name: test ppl name\n",
      "Pipeline ID: cf2d56ea-d345-4f2a-bf71-95e935224ebe\n",
      "Pipeline Name: test ppl name02\n",
      "Pipeline ID: fcb42b6e-66f1-4e32-8a11-0faeed19d81b\n",
      "Pipeline Name: nnn\n",
      "Pipeline ID: 903f88fc-2255-488e-9668-d806808948a2\n",
      "Pipeline Name: nnncc\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import Client\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "# 列出所有的 pipelines\n",
    "pipelines = client.list_pipelines()\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(\"Pipeline ID:\", pipeline._pipeline_id)\n",
    "    print(\"Pipeline Name:\", pipeline._display_name)\n",
    "\n",
    "    # 获取并打印该 pipeline 的所有版本信息\n",
    "    versions = client.list_pipeline_versions(pipeline_id=pipeline._pipeline_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5586c69-b289-44ce-bb17-a8f260195dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ID: 97838453-7a91-42b8-a277-c4c550a9e2c4\n",
      "Pipeline Name: [Tutorial] Data passing in python components\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: c74e3130-3806-4955-a299-c32f1e22bccd\n",
      "Pipeline Name: [Tutorial] DSL - Control structures\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: d0427560-86fd-42f1-a42b-314195292746\n",
      "Pipeline Name: test ppl name\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: cf2d56ea-d345-4f2a-bf71-95e935224ebe\n",
      "Pipeline Name: test ppl name02\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: fcb42b6e-66f1-4e32-8a11-0faeed19d81b\n",
      "Pipeline Name: nnn\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: 903f88fc-2255-488e-9668-d806808948a2\n",
      "Pipeline Name: nnncc\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n",
      "Pipeline ID: 2ccfdbd0-caf3-4383-843a-3961dfd24c8e\n",
      "Pipeline Name: sample-pipeline\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_next_page_token', '_pipeline_versions', '_total_size', 'attribute_map', 'discriminator', 'local_vars_configuration', 'next_page_token', 'openapi_types', 'pipeline_versions', 'to_dict', 'to_str', 'total_size']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import Client\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "# 列出所有的 pipelines\n",
    "pipelines = client.list_pipelines()\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(\"Pipeline ID:\", pipeline._pipeline_id)\n",
    "    print(\"Pipeline Name:\", pipeline._display_name)\n",
    "\n",
    "    # 获取该 pipeline 的版本信息\n",
    "    versions_response = client.list_pipeline_versions(pipeline_id=pipeline._pipeline_id)\n",
    "    print(dir(versions_response))  # 打印所有属性和方法，以便了解如何正确访问版本信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d21f9ac-84ab-42f8-aaac-44b28c91fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_code_source_url', '_created_at', '_description', '_display_name', '_error', '_package_url', '_pipeline_id', '_pipeline_spec', '_pipeline_version_id', 'attribute_map', 'code_source_url', 'created_at', 'description', 'discriminator', 'display_name', 'error', 'local_vars_configuration', 'openapi_types', 'package_url', 'pipeline_id', 'pipeline_spec', 'pipeline_version_id', 'to_dict', 'to_str']\n"
     ]
    }
   ],
   "source": [
    "if hasattr(versions_response, 'pipeline_versions') and versions_response.pipeline_versions:\n",
    "    version_example = versions_response.pipeline_versions[0]\n",
    "    print(dir(version_example))\n",
    "else:\n",
    "    print(\"No versions or unable to access a version example.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef0a4f7c-d664-4c85-9482-217ef7736c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ID for 'nnncc': 903f88fc-2255-488e-9668-d806808948a2\n",
      "Version ID: 5628dfd8-4e10-4afd-8007-87f1c1bd7e21, Version Name: nnncc\n",
      "5628dfd8-4e10-4afd-8007-87f1c1bd7e21\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import Client\n",
    "\n",
    "# Connect to Kubeflow Pipelines\n",
    "client = kfp.Client()\n",
    "\n",
    "# Get the pipeline ID by name\n",
    "pipeline_name = 'nnncc'\n",
    "pipeline_id = client.get_pipeline_id(pipeline_name)\n",
    "print(f\"Pipeline ID for '{pipeline_name}': {pipeline_id}\")\n",
    "\n",
    "# Get all versions for the specified pipeline\n",
    "versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "\n",
    "\n",
    "# Print each version's ID and Display Name\n",
    "if versions_response.pipeline_versions:\n",
    "    for version in versions_response.pipeline_versions:\n",
    "        # 使用 _pipeline_version_id 属性获取版本 ID\n",
    "        print(f\"Version ID: {version._pipeline_version_id}, Version Name: {version.display_name}\")\n",
    "else:\n",
    "    print(\"No versions found for the given pipeline.\")\n",
    "\n",
    "print(versions_response.pipeline_versions[0]._pipeline_version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27499513-d939-43dc-a523-97f8d0ffdcc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_id:None\n"
     ]
    },
    {
     "ename": "ApiValueError",
     "evalue": "Missing the required parameter `pipeline_id` when calling `list_pipeline_versions`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiValueError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_id:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# --> same\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"pipeline._pipeline_id:{pipeline._pipeline_id}\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get all versions for the specified pipeline\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m versions_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_pipeline_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m version_id \u001b[38;5;241m=\u001b[39m versions_response\u001b[38;5;241m.\u001b[39mpipeline_versions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_pipeline_version_id\n\u001b[1;32m     18\u001b[0m input_dict_parameter \u001b[38;5;241m=\u001b[39m {k: randint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sample(ascii_letters, \u001b[38;5;241m10\u001b[39m)}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:1528\u001b[0m, in \u001b[0;36mClient.list_pipeline_versions\u001b[0;34m(self, pipeline_id, page_token, page_size, sort_by, filter)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_pipeline_versions\u001b[39m(\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1498\u001b[0m     pipeline_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mfilter\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1503\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m kfp_server_api\u001b[38;5;241m.\u001b[39mV2beta1ListPipelineVersionsResponse:\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lists pipeline versions.\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;124;03m        ``V2beta1ListPipelineVersionsResponse`` object.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipelines_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_pipeline_versions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/pipeline_service_api.py:1101\u001b[0m, in \u001b[0;36mPipelineServiceApi.list_pipeline_versions\u001b[0;34m(self, pipeline_id, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lists all pipeline versions of a given pipeline ID.  # noqa: E501\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m:rtype: V2beta1ListPipelineVersionsResponse\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_pipeline_versions_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/pipeline_service_api.py:1170\u001b[0m, in \u001b[0;36mPipelineServiceApi.list_pipeline_versions_with_http_info\u001b[0;34m(self, pipeline_id, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# verify the required parameter 'pipeline_id' is set\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mclient_side_validation \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline_id\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m local_var_params \u001b[38;5;129;01mor\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m                                                 local_var_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiValueError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing the required parameter `pipeline_id` when calling `list_pipeline_versions`\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m collection_formats \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1174\u001b[0m path_params \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mApiValueError\u001b[0m: Missing the required parameter `pipeline_id` when calling `list_pipeline_versions`"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import Client\n",
    "from random import randint, choice, sample\n",
    "from string import ascii_letters\n",
    "\n",
    "\n",
    "# connect to Kubeflow Pipelines\n",
    "client = kfp.Client()\n",
    "\n",
    "# check pipeline id\n",
    "pipeline_id = client.get_pipeline_id('nnncc')\n",
    "print(f\"pipeline_id:{pipeline_id}\")  # --> same\n",
    "# print(f\"pipeline._pipeline_id:{pipeline._pipeline_id}\")\n",
    "# Get all versions for the specified pipeline\n",
    "versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "version_id = versions_response.pipeline_versions[0]._pipeline_version_id\n",
    "\n",
    "input_dict_parameter = {k: randint(1, 100) for k in sample(ascii_letters, 10)}\n",
    "input_list_parameter = [''.join(choice(ascii_letters) for _ in range(10)) for _ in range(5)]\n",
    "\n",
    "\n",
    "# set Pipeline params\n",
    "params = {\n",
    "    'message': 'Hello, this is a test. This is a complex test message with multiple characters and numbers 1234567890',\n",
    "    # 'input_dict_parameter': input_dict_parameter,\n",
    "    # 'input_list_parameter': input_list_parameter\n",
    "}\n",
    "print(f\"params:{params}\")   \n",
    "# create an experiment\n",
    "experiment = client.create_experiment(name='Test Experiment')\n",
    "print(f\"experiment.experiment_id:{experiment.experiment_id}\") \n",
    "\n",
    "# run  Pipeline, 'My Pipeline Run' is run name\n",
    "# run = client.run_pipeline(experiment.experiment_id, 'My Pipeline Run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "try:\n",
    "    run = client.run_pipeline(experiment.experiment_id, 'My Pipeline Run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to start pipeline run: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e41c2fd0-29a5-41d4-b7e2-ea4d5b605dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7f5096b09c50>\n",
      "Successfully compiled to yaml...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/e8e056b5-9d65-436d-8f97-2d628413b178\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment created with ID==>e8e056b5-9d65-436d-8f97-2d628413b178\n",
      "Failed upload the pipeline: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'date': 'Tue, 25 Jun 2024 10:12:00 GMT', 'content-length': '505', 'content-type': 'text/plain; charset=utf-8', 'x-envoy-upstream-service-time': '63', 'server': 'envoy'})\n",
      "HTTP response body: {\"error_message\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name hello-world-csv-pipeline already exists. Please specify a new name\",\"error_details\":\"Failed to create a pipeline and a pipeline version: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name hello-world-csv-pipeline already exists. Please specify a new name\"}\n",
      "\n",
      "Failed to run the pipeline: name 'version_id' is not defined\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def getdata(url: str, prepared_csv: OutputPath()) -> None:\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    # get data from url and unzip\n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n",
    "    \n",
    "    #read unzip data into dataframe\n",
    "    df = pd.concat([pd.read_csv(csv_file, header=None)\n",
    "                    for csv_file in glob.glob('data/*.csv')])\n",
    "    with open(prepared_csv, \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print('--- inside getdata ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "# def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath())-> None:\n",
    "def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath()):\n",
    "    import pandas as pd\n",
    "    print(f\"csv_in_file: {csv_in_file}\")\n",
    "    \n",
    "    with open(csv_in_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    # preprocess here\n",
    "    with open(preprocessed_output_csv, 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    print('--- inside preprocess ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def train(csv_in_file: InputPath()):\n",
    "\n",
    "    with open(csv_in_file) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    print('--- inside train ---')\n",
    "    print(f\"df: {df}\")\n",
    "    \n",
    "\n",
    "\n",
    "@dsl.pipeline(name='my-test-pipeline-beta')\n",
    "def pipeline(url: str):\n",
    "    prepare_data_task = getdata(url=url)\n",
    "    print(prepare_data_task)\n",
    "    preprocess_task = preprocess(\n",
    "        csv_in_file=prepare_data_task.outputs['prepared_csv']\n",
    "    )\n",
    "#     prepare_data = getdata(url=url)\n",
    "#     print(prepare_data)\n",
    "\n",
    "#     preprocess_data = preprocess(\n",
    "#         csv_in_file = prepare_data.outputs['prepared_csv']  \n",
    "#     )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "namespace='kubeflow-user-example-com'\n",
    "project_name='hello-world-csv'\n",
    "\n",
    "client = kfp.Client()\n",
    "    \n",
    "#Compile the pipeline to YAML\n",
    "try:\n",
    "    yaml_file = os.path.join(os.getcwd(), project_name+'.yaml')\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "    print(f\"Successfully compiled to yaml...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to compile to YAML: {e}\")\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "try:\n",
    "    experiment = client.create_experiment(name=project_name+'-expr', namespace=namespace)\n",
    "    print(f\"The experiment created with ID==>{experiment.experiment_id}\") \n",
    "except Exception as e:\n",
    "    print(f\"Failed to Create the experiment: {e}\")\n",
    "\n",
    "\n",
    "#Upload the pipeline\n",
    "try:\n",
    "    \n",
    "    #Create shared pipeline\n",
    "    #pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name='sample-pipeline')\n",
    "    \n",
    "    #Create private pipeline\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name=project_name+'-pipeline', namespace=namespace)\n",
    "    \n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    \n",
    "    versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "    version_id = versions_response.pipeline_versions[0]._pipeline_version_id\n",
    "    \n",
    "    print(f\"The yaml file uploaded to a new pipeline with PipelineID: {pipeline_id}, PipelineVersion: {version_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed upload the pipeline: {e}\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "}\n",
    "\n",
    "\n",
    "#Create a RUN\n",
    "try:\n",
    "    run = client.run_pipeline(experiment.experiment_id, project_name+'-run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to run the pipeline: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5365762-7671-4769-8e44-9cab1269708f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_986/3693463898.py:179: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(exists.output == 'True', name='Data_Exists'):\n",
      "/tmp/ipykernel_986/3693463898.py:186: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(exists.output == 'False', name='Data_Not_Exists'):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got unknown pipeline output: <kfp.dsl.pipeline_task.PipelineTask object at 0x7f50966ee150>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 169\u001b[0m\n\u001b[1;32m    164\u001b[0m         response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    165\u001b[0m         response\u001b[38;5;241m.\u001b[39mrelease_conn()\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;129;43m@dsl\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m   \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcensus-pipeline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPipeline that will download Census data and save to MinIO.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    172\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mcensus_pipeline\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_code\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# initiate result param\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# result_table_data = None\u001b[39;49;00m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexists\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtable_data_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdsl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexists\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData_Exists\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/dsl/pipeline_context.py:65\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     63\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/dsl/component_factory.py:669\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    665\u001b[0m     func,\n\u001b[1;32m    666\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    667\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    668\u001b[0m )\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/dsl/graph_component.py:68\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name)\u001b[0m\n\u001b[1;32m     65\u001b[0m pipeline_group \u001b[38;5;241m=\u001b[39m dsl_pipeline\u001b[38;5;241m.\u001b[39mgroups[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     66\u001b[0m pipeline_group\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n\u001b[0;32m---> 68\u001b[0m pipeline_spec, platform_spec \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipeline_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdsl_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m pipeline_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(pipeline_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline_root\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/compiler/pipeline_spec_builder.py:1838\u001b[0m, in \u001b[0;36mcreate_pipeline_spec\u001b[0;34m(pipeline, component_spec, pipeline_outputs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m pipeline_spec\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mCopyFrom(\n\u001b[1;32m   1833\u001b[0m     _build_component_spec_from_component_spec_structure(component_spec))\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;66;03m# TODO: add validation of returned outputs -- it's possible to return\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;66;03m# an output from a task in a condition group, for example, which isn't\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;66;03m# caught until submission time using Vertex SDK client\u001b[39;00m\n\u001b[0;32m-> 1838\u001b[0m pipeline_outputs_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_pipeline_outputs_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m validate_pipeline_outputs_dict(pipeline_outputs_dict)\n\u001b[1;32m   1841\u001b[0m root_group \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mgroups[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/compiler/pipeline_spec_builder.py:1941\u001b[0m, in \u001b[0;36mconvert_pipeline_outputs_to_dict\u001b[0;34m(pipeline_outputs)\u001b[0m\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(pipeline_outputs\u001b[38;5;241m.\u001b[39m_asdict())\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot unknown pipeline output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_outputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Got unknown pipeline output: <kfp.dsl.pipeline_task.PipelineTask object at 0x7f50966ee150>"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "# from kfp import dsl\n",
    "# from kfp.dsl import component\n",
    "# from kfp.dsl import Input\n",
    "# from kfp.dsl import Output\n",
    "from kfp.dsl import Dataset\n",
    "\n",
    "import io\n",
    "import logging\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component, Output, Input\n",
    "\n",
    "\n",
    "@component(packages_to_install=['minio==7.1.14'])\n",
    "# def table_data_exists(bucket: str, table_code: str, year: int) -> bool:\n",
    "def table_data_exists(bucket: str, table_code: str, year: int) -> Output[bool]:\n",
    "    \"\"\"\n",
    "   \n",
    "    Check for the existence of Census table data in MinIO.\n",
    "    \"\"\"\n",
    "    object_name=f'{table_code}-{year}.csv'\n",
    "\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(bucket)\n",
    "    logger.info(table_code)\n",
    "    logger.info(year)\n",
    "    logger.info(object_name)\n",
    "  \n",
    "    try:\n",
    "        # Create client with access and secret key.\n",
    "        client = Minio('10.0.84.43:80',\n",
    "                   'minio',\n",
    "                   'minio123',\n",
    "                   secure=False)\n",
    "\n",
    "        bucket_found = client.bucket_exists(bucket)\n",
    "        if not bucket_found:\n",
    "            return False\n",
    "\n",
    "        objects = client.list_objects(bucket)\n",
    "        found = False\n",
    "        for obj in objects:\n",
    "            logger.info(obj.object_name)\n",
    "            if object_name == obj.object_name: found = True\n",
    "\n",
    "    except S3Error as s3_err:\n",
    "        logger.error(f'S3 Error occurred: {s3_err}.')\n",
    "    except Error as err:\n",
    "        logger.error(f'Error occurred: {err}.')\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "@component(packages_to_install=['pandas==1.3.5', 'requests'])\n",
    "def download_table_data(dataset: str, table_code: str, year: int, table_df: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Returns all fields for the specified table. The output is a DataFrame saved to csv.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    census_endpoint = f'https://api.census.gov/data/{year}/{dataset}'\n",
    "    census_key = 'Census key here.'\n",
    "   \n",
    "   # Setup a simple dictionary for the requests parameters.\n",
    "    get_token = f'group({table_code})'\n",
    "    params = {'key': census_key,\n",
    "             'get': get_token,\n",
    "             'for': 'county:*'\n",
    "             }\n",
    "\n",
    "    # sending get request and saving the response as response object\n",
    "    response = requests.get(url=census_endpoint, params=params)\n",
    "   \n",
    "   # Extract the data in json format.\n",
    "   # The first row of our matrix contains the column names. The remaining rows\n",
    "   # are the data.\n",
    "    survey_data = response.json()\n",
    "    df = pd.DataFrame(survey_data[1:], columns = survey_data[0])\n",
    "    df.to_csv(table_df.path, index=False)\n",
    "    logger.info(f'Table {table_code} for {year} has been downloaded.')\n",
    "\n",
    "\n",
    "@component(packages_to_install=['pandas==1.3.5', 'minio==7.1.14'])\n",
    "def save_table_data(bucket: str, table_code: str, year: int, table_df: Input[Dataset]):\n",
    "    \n",
    "\n",
    "    object_name=f'{table_code}-{year}.csv'\n",
    "\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(bucket)\n",
    "    logger.info(table_code)\n",
    "    logger.info(year)\n",
    "    logger.info(object_name)\n",
    "\n",
    "    df = pd.read_csv(table_df.path)\n",
    "\n",
    "    try:\n",
    "       # Create client with access and secret key\n",
    "        client = Minio('10.0.84.43:80',\n",
    "                   'minio',\n",
    "                   'minio123',\n",
    "                   secure=False)\n",
    "\n",
    "       # Make the bucket if it does not exist.\n",
    "        found = client.bucket_exists(bucket)\n",
    "        if not found:\n",
    "            logger.info(f'Creating bucket: {bucket}.')\n",
    "            client.make_bucket(bucket)\n",
    "\n",
    "       # Upload the dataframe as an object.\n",
    "        encoded_df = df.to_csv(index=False).encode('utf-8')\n",
    "        client.put_object(bucket, object_name, data=io.BytesIO(encoded_df), length=len(encoded_df), content_type='application/csv')\n",
    "        logger.info(f'{object_name} successfully uploaded to bucket {bucket}.')\n",
    "        logger.info(f'Object length: {len(df)}.')\n",
    "\n",
    "    except S3Error as s3_err:\n",
    "        logger.error(f'S3 Error occurred: {s3_err}.')\n",
    "    except Error as err:\n",
    "        logger.error(f'Error occurred: {err}.')\n",
    "\n",
    "\n",
    "@component(packages_to_install=['pandas==1.3.5', 'minio==7.1.14'])\n",
    "def get_table_data(bucket: str, table_code: str, year: int, table_df: Output[Dataset]):\n",
    "\n",
    "    object_name=f'{table_code}-{year}.csv'\n",
    "\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(bucket)\n",
    "    logger.info(table_code)\n",
    "    logger.info(year)\n",
    "    logger.info(object_name)\n",
    "\n",
    "    # Get data of an object.\n",
    "    try:\n",
    "        \n",
    "        # Create client with access and secret key\n",
    "        client = Minio('10.0.84.43:80',\n",
    "                   'minio',\n",
    "                   'minio123',\n",
    "                   secure=False)\n",
    "\n",
    "        response = client.get_object(bucket, object_name)\n",
    "        df = pd.read_csv(io.BytesIO(response.data))\n",
    "        df.to_csv(table_df.path, index=False)\n",
    "        logger.info(f'Object: {object_name} has been retrieved from bucket: {bucket} in MinIO object storage.')\n",
    "        logger.info(f'Object length: {len(df)}.')\n",
    "\n",
    "    except S3Error as s3_err:\n",
    "        logger.error(f'S3 Error occurred: {s3_err}.')\n",
    "    except Error as err:\n",
    "        logger.error(f'Error occurred: {err}.')\n",
    "\n",
    "    finally:\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "\n",
    "        \n",
    "\n",
    "@dsl.pipeline(\n",
    "   name='census-pipeline',\n",
    "   description='Pipeline that will download Census data and save to MinIO.'\n",
    ")\n",
    "def census_pipeline(bucket: str, dataset: str, table_code: str, year: int):\n",
    "\n",
    "    # initiate result param\n",
    "    # result_table_data = None\n",
    "    exists = table_data_exists(bucket=bucket, table_code=table_code, year=year)\n",
    "\n",
    "    with dsl.Condition(exists.output == 'True', name='Data_Exists'): \n",
    "        table_data = download_table_data(dataset=dataset, table_code=table_code, year=year)\n",
    "        save_table_data(bucket=bucket,\n",
    "                      table_code=table_code,\n",
    "                      year=year,\n",
    "                      table_df=table_data.outputs['table_df'])\n",
    "        result_table_data = table_data.outputs['table_df']\n",
    "    with dsl.Condition(exists.output == 'False', name='Data_Not_Exists'):\n",
    "        result_table_data = get_table_data(bucket=bucket, table_code=table_code, year=year)        \n",
    "\n",
    "    return result_table_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8ed0bc-b3b5-401f-a471-275df51863c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7efeb163c3d0>\n",
      "Successfully compiled to yaml...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/65bf4d7d-abba-4fa1-ae48-30e056e089b1\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment created with ID==>65bf4d7d-abba-4fa1-ae48-30e056e089b1\n",
      "The yaml file uploaded to a new pipeline with PipelineID: 119e60ec-05b4-4739-8009-87bd11a6765d, PipelineVersion: 655f494f-7c55-4a8a-84c1-0459b0ffd623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/ede78dc1-7a4b-4a83-8242-9bc866d0562e\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: ede78dc1-7a4b-4a83-8242-9bc866d0562e\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def getdata(url: str, prepared_csv: OutputPath()) -> None:\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    # get data from url and unzip\n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n",
    "    \n",
    "    #read unzip data into dataframe\n",
    "    df = pd.concat([pd.read_csv(csv_file, header=None)\n",
    "                    for csv_file in glob.glob('data/*.csv')])\n",
    "    with open(prepared_csv, \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print('--- inside getdata ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "# def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath())-> None:\n",
    "def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath()):\n",
    "    import pandas as pd\n",
    "    print(f\"csv_in_file: {csv_in_file}\")\n",
    "    \n",
    "    with open(csv_in_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    # preprocess here\n",
    "    with open(preprocessed_output_csv, 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    print('--- inside preprocess ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def train(csv_in_file: InputPath()):\n",
    "    import pandas as pd\n",
    "\n",
    "    with open(csv_in_file) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    with open('minio://mlpipeline/v2/artifacts/test-pipleine2/test/out_put.csv', 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "        \n",
    "    print('--- inside train ---')\n",
    "    print(f\"df: {df}\")\n",
    "    \n",
    "\n",
    "\n",
    "@dsl.pipeline(name='test_pipleine2')\n",
    "def pipeline(url: str):\n",
    "    data_from_source = getdata(url=url)\n",
    "    print(data_from_source)\n",
    "    preprocessed_data = preprocess(\n",
    "        csv_in_file=data_from_source.outputs['prepared_csv']\n",
    "    )\n",
    "    trained_data = train(\n",
    "        csv_in_file=preprocessed_data.outputs['preprocessed_output_csv']\n",
    "    )\n",
    "#     prepare_data = getdata(url=url)\n",
    "#     print(prepare_data)\n",
    "\n",
    "#     preprocess_data = preprocess(\n",
    "#         csv_in_file = prepare_data.outputs['prepared_csv']  \n",
    "#     )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "namespace='kubeflow-user-example-com'\n",
    "project_name='test project name gcc2'\n",
    "\n",
    "client = kfp.Client()\n",
    "    \n",
    "#Compile the pipeline to YAML\n",
    "try:\n",
    "    yaml_file = os.path.join(os.getcwd(), project_name+'.yaml')\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "    print(f\"Successfully compiled to yaml...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to compile to YAML: {e}\")\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "try:\n",
    "    experiment = client.create_experiment(name=project_name+'-expr', namespace=namespace)\n",
    "    print(f\"The experiment created with ID==>{experiment.experiment_id}\") \n",
    "except Exception as e:\n",
    "    print(f\"Failed to Create the experiment: {e}\")\n",
    "\n",
    "\n",
    "#Upload the pipeline\n",
    "try:\n",
    "    \n",
    "    #Create shared pipeline\n",
    "    #pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name='sample-pipeline')\n",
    "    \n",
    "    #Create private pipeline\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name=project_name+'-pipeline', namespace=namespace)\n",
    "    \n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    \n",
    "    versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "    version_id = versions_response.pipeline_versions[0]._pipeline_version_id\n",
    "    \n",
    "    print(f\"The yaml file uploaded to a new pipeline with PipelineID: {pipeline_id}, PipelineVersion: {version_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed upload the pipeline: {e}\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "}\n",
    "\n",
    "\n",
    "#Create a RUN\n",
    "try:\n",
    "    run = client.run_pipeline(experiment.experiment_id, project_name+'-run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to run the pipeline: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa3ed4b-9f70-4836-8c48-cbd5c7723453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7f4f9d80bd50>\n",
      "Successfully compiled to yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n",
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/8f207a51-74c6-4429-9fcb-94cf481081eb\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment created with ID==>8f207a51-74c6-4429-9fcb-94cf481081eb\n",
      "The yaml file uploaded to a new pipeline with PipelineID: 78bae54c-c97f-4c40-80e9-e2b3d529bf9f, PipelineVersion: 3c3e725d-07aa-4c92-ac81-3b0a6157e3f6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/a1b0d166-b2c6-433c-9aa7-839583d26a62\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: a1b0d166-b2c6-433c-9aa7-839583d26a62\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def getdata(url: str, prepared_csv: OutputPath()) -> None:\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    # get data from url and unzip\n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n",
    "    \n",
    "    #read unzip data into dataframe\n",
    "    df = pd.concat([pd.read_csv(csv_file, header=None)\n",
    "                    for csv_file in glob.glob('data/*.csv')])\n",
    "    with open(prepared_csv, \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print('--- inside getdata ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "# def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath())-> None:\n",
    "def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath()):\n",
    "    import pandas as pd\n",
    "    print(f\"csv_in_file: {csv_in_file}\")\n",
    "    \n",
    "    with open(csv_in_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    # preprocess here\n",
    "    with open(preprocessed_output_csv, 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    print('--- inside preprocess ---')\n",
    "    print(f\"df: {df}\")\n",
    "\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"pandas\", \"minio\"])\n",
    "def train(csv_in_file: InputPath(), bucket_name: str, object_name: str):\n",
    "    import pandas as pd\n",
    "    from minio import Minio\n",
    "    from minio.error import S3Error\n",
    "    import io\n",
    "    client = Minio(\n",
    "        \"10.0.84.43:80\",\n",
    "        access_key='minio',\n",
    "        secret_key='minio123',\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "\n",
    "    with open(csv_in_file) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    # csv_str = df.to_csv(index=False)\n",
    "    csv_bytes = io.BytesIO(df.to_csv(index=False).encode('utf-8'))\n",
    "    try:\n",
    "        client.put_object(\n",
    "            bucket_name,\n",
    "            object_name,\n",
    "            data=csv_bytes,\n",
    "            length=csv_bytes.getbuffer().nbytes\n",
    "        )\n",
    "    except S3Error as exc:\n",
    "        print(\"Error occurred while uploading to MinIO:\", exc)\n",
    "    \n",
    "        \n",
    "    print('--- inside train ---')\n",
    "    print(f\"df: {df}\")\n",
    "    \n",
    "\n",
    "\n",
    "@dsl.pipeline(name='test_pipleine2')\n",
    "def pipeline(url: str):\n",
    "    data_from_source = getdata(url=url)\n",
    "    print(data_from_source)\n",
    "    preprocessed_data = preprocess(\n",
    "        csv_in_file=data_from_source.outputs['prepared_csv']\n",
    "    )\n",
    "    trained_data = train(\n",
    "        csv_in_file=preprocessed_data.outputs['preprocessed_output_csv'],\n",
    "        bucket_name = 'mlpipeline',\n",
    "        object_name=f'outputtest.csv'\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "namespace='kubeflow-user-example-com'\n",
    "project_name='test project name gcc2'\n",
    "\n",
    "client = kfp.Client()\n",
    "    \n",
    "#Compile the pipeline to YAML\n",
    "try:\n",
    "    yaml_file = os.path.join(os.getcwd(), project_name+'.yaml')\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "    print(f\"Successfully compiled to yaml...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to compile to YAML: {e}\")\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "try:\n",
    "    experiment = client.create_experiment(name=project_name+'-expr', namespace=namespace)\n",
    "    print(f\"The experiment created with ID==>{experiment.experiment_id}\") \n",
    "except Exception as e:\n",
    "    print(f\"Failed to Create the experiment: {e}\")\n",
    "\n",
    "\n",
    "#Upload the pipeline\n",
    "try:\n",
    "    \n",
    "    #Create shared pipeline\n",
    "    #pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name='sample-pipeline')\n",
    "    \n",
    "    #Create private pipeline\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name=project_name+'-pipeline', namespace=namespace)\n",
    "    \n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    \n",
    "    versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "    version_id = versions_response.pipeline_versions[0]._pipeline_version_id\n",
    "    \n",
    "    print(f\"The yaml file uploaded to a new pipeline with PipelineID: {pipeline_id}, PipelineVersion: {version_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed upload the pipeline: {e}\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "}\n",
    "\n",
    "\n",
    "#Create a RUN\n",
    "try:\n",
    "    run = client.run_pipeline(experiment.experiment_id, project_name+'-run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to run the pipeline: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c23047-0a76-42c8-867e-a9aef900777a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769354e2-28c8-4521-a778-dc7a78491836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a3a72-a9fe-44b3-9f4d-243128106bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44110316-8fbd-46fb-9fd1-186c7ea73652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de061ea9-d9f2-498e-9a2e-b6dc4e2db781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
