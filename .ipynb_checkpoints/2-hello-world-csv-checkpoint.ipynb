{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e578328-f390-4778-8956-ac3c0d7e1d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully compiled to yaml...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/513e25d4-2ac5-4de5-8e4f-5450d5126d6b\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment created with ID==>513e25d4-2ac5-4de5-8e4f-5450d5126d6b\n",
      "The yaml file uploaded to a new pipeline with PipelineID: d7ad2172-c3aa-4f87-88fa-818a43f96323, PipelineVersion: a9b0fe49-1bda-4252-9b63-9df14d687064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/b2a2a4de-c0a4-4042-a788-2e67e20907ea\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: b2a2a4de-c0a4-4042-a788-2e67e20907ea\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import OutputPath\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def getdata(url: str, prepared_csv: OutputPath()) -> None:\n",
    "    \n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "\n",
    "    print('--- inside getdata ---')\n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n",
    "    df = pd.concat([pd.read_csv(csv_file, header=None)\n",
    "                    for csv_file in glob.glob('data/*.csv')])\n",
    "    with open(prepared_csv, \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"pandas\"])\n",
    "def preprocess(csv_in_file: InputPath(), preprocessed_output_csv: OutputPath()) -> None:\n",
    "    import pandas as pd\n",
    "    \n",
    "    print('--- inside preprocess ---')\n",
    "    with open(csv_in_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    \n",
    "    with open(preprocessed_output_csv, 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "@component(packages_to_install=[\"pandas\",\"minio\"])\n",
    "def train(csv_in_file: InputPath(), bucket_name: str, object_name: str):\n",
    "    import pandas as pd\n",
    "    from minio import Minio\n",
    "    from minio.error import S3Error\n",
    "    import io\n",
    "    \n",
    "    print('--- inside train ---')\n",
    "    \n",
    "    client = Minio(\n",
    "        \"10.0.84.43:80\",\n",
    "        access_key='minio',\n",
    "        secret_key='minio123',\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    with open(csv_in_file) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    csv_bytes = io.BytesIO(df.to_csv(index=False).encode('utf-8'))\n",
    "    try:\n",
    "        found = client.bucket_exists(bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name)\n",
    "        client.put_object(\n",
    "            bucket_name,\n",
    "            object_name,\n",
    "            data=csv_bytes,\n",
    "            length=csv_bytes.getbuffer().nbytes\n",
    "        )\n",
    "    except S3Error as exc:\n",
    "        print(\"Error occurred while uploading to MinIO:\", exc)\n",
    "        \n",
    "        \n",
    "\n",
    "namespace='kubeflow-user-example-com'\n",
    "project_name='hello-csv'\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "\n",
    "@dsl.pipeline(name= project_name)\n",
    "def pipeline(url: str):\n",
    "    get_data = getdata(url=url)\n",
    "    preprocess_data = preprocess(\n",
    "        csv_in_file=get_data.outputs['prepared_csv']\n",
    "    )\n",
    "    train_data = train(\n",
    "        csv_in_file=preprocess_data.outputs['preprocessed_output_csv'],\n",
    "        bucket_name = 'hello-world-csv-bucket',\n",
    "        object_name=f'trained_data2.csv'\n",
    "    )\n",
    "\n",
    "\n",
    "#Compile the pipeline to YAML\n",
    "try:\n",
    "    yaml_file = os.path.join(os.getcwd(), project_name+'.yaml')\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=yaml_file)\n",
    "    print(f\"Successfully compiled to yaml...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to compile to YAML: {e}\")\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "try:\n",
    "    experiment = client.create_experiment(name=project_name+'-expr', namespace=namespace)\n",
    "    print(f\"The experiment created with ID==>{experiment.experiment_id}\") \n",
    "except Exception as e:\n",
    "    print(f\"Failed to Create the experiment: {e}\")\n",
    "\n",
    "\n",
    "#Upload the pipeline\n",
    "try:\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(yaml_file, name=project_name+'-pipeline', namespace=namespace)\n",
    "    \n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    \n",
    "    versions_response = client.list_pipeline_versions(pipeline_id=pipeline_id)\n",
    "    version_id = versions_response.pipeline_versions[0]._pipeline_version_id\n",
    "    \n",
    "    print(f\"The yaml file uploaded to a new pipeline with PipelineID: {pipeline_id}, PipelineVersion: {version_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed upload the pipeline: {e}\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "}\n",
    "\n",
    "\n",
    "#Create a RUN\n",
    "try:\n",
    "    run = client.run_pipeline(experiment.experiment_id, project_name+'-run', pipeline_id=pipeline_id, params=params, version_id=version_id)\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to run the pipeline: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0b7e4-8acb-4138-ba02-3cafc6a05c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
